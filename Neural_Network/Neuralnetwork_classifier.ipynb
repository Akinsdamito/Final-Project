{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from python_functions import *\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_filepath):\n",
    "    \n",
    "    \"\"\"\n",
    "        This function takes in the database path and it reads the data\n",
    "        \n",
    "        \n",
    "        return: The messages which are the predators, the response i.e the labels as a dataframe \n",
    "            and an array of all the labels\n",
    "    \"\"\"\n",
    "    df=pd.read_csv(data_filepath, sep=\",\", header=0)\n",
    "    new_df=df[df['sent_length']>20].dropna(subset=['clean_headline']).reset_index(drop=True)\n",
    "    \n",
    "    X_data_set=new_df['clean_headline']#.tolist()\n",
    "    Y_data_set = new_df.drop(['clean_headline','sent_length'], axis=1)\n",
    "    label_name=Y_data_set.columns\n",
    "    return X_data_set,Y_data_set,label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, category_names=load_data('../model_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python functions for NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.df=df\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        for line in self.df:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyCorpus at 0x12918a717c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = MyCorpus(X)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=sentences,min_count=2,vector_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store just the words + their trained embeddings.\n",
    "word_vectors = model.wv\n",
    "\n",
    "word_vectors.save_word2vec_format('test_w2v.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a word2vec model stored in the C *text* format.\n",
    "\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(\"test_w2v.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_w2v.txt') as f:\n",
    "    data1 =f.readlines()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Word2vec vectors...\n",
      "Found 10816 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 200                                          \n",
    "max_words = 8000\n",
    "\n",
    "# apply the vectors provided by Word2Vec to create a word embedding matrix\n",
    "print(\"Applying Word2vec vectors...\")\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "for line in data1:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Narative components data...\n",
      "Loading Narative components data completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akino\\anaconda3\\envs\\DeepLearning_tf\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1 2 3], y=8157     3\n",
      "1332     2\n",
      "9129     1\n",
      "5149     0\n",
      "7293     1\n",
      "        ..\n",
      "11964    3\n",
      "21575    1\n",
      "5390     0\n",
      "860      3\n",
      "15795    3\n",
      "Name: category, Length: 15380, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "## Creating dictionary of the label and category number\n",
    "\n",
    "news_labels=Y['category'].unique()\n",
    "news_labels_dict={}\n",
    "for index in range(len(news_labels)):\n",
    "    news_labels_dict[news_labels[index]]=index    \n",
    "\n",
    "print(\"Loading Narative components data...\")\n",
    "\n",
    "headline =X\n",
    "\n",
    "## Changing label to categorical values\n",
    "labels = Y['category'].apply(lambda x: news_labels_dict[x])\n",
    "print(\"Loading Narative components data completed.\")\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, Y_train, y_test = train_test_split(headline,labels,test_size=0.3,random_state=42)\n",
    "\n",
    "## Calculation of the class weight\n",
    "weights= compute_class_weight(\n",
    "           'balanced',\n",
    "            np.unique(Y_train), \n",
    "            Y_train)\n",
    "\n",
    "weights_dict = dict(zip( np.unique(Y_train),weights))\n",
    "\n",
    "y_train_data = to_categorical(Y_train)\n",
    "y_test_dat = to_categorical(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n",
      "Found 14825 unique tokens.\n",
      "Shape of data tensor: (15380, 200)\n",
      "Shape of label tensor: (21972,)\n",
      "Tokenizing data complete.\n",
      "Applying word2vec vectors completed.\n"
     ]
    }
   ],
   "source": [
    "# use the Tokenizer from Keras to \"learn\" a vocabulary from the entire car components text\n",
    "print(\"Tokenizing data...\")    \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Padding the dataset to have same dimention of the embedding matrix\n",
    "x_train_seq = pad_sequences(train_sequences, maxlen=embedding_dim)\n",
    "x_test_seq = pad_sequences(test_sequences, maxlen=embedding_dim)\n",
    "\n",
    "labels = np.asarray(labels, dtype='float32')\n",
    "print('Shape of data tensor:', x_train_seq.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "print(\"Tokenizing data complete.\")\n",
    "\n",
    "## Applying word embedding \n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector    \n",
    "print(\"Applying word2vec vectors completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model structure...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 200)          1600000   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               4000100   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 244       \n",
      "=================================================================\n",
      "Total params: 5,606,404\n",
      "Trainable params: 5,606,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Creating model structure completed.\n",
      "Training model...\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 5s 46ms/step - loss: 1.3972 - acc: 0.2369 - val_loss: 1.3766 - val_acc: 0.3643\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 7s 61ms/step - loss: 1.3943 - acc: 0.2255 - val_loss: 1.3898 - val_acc: 0.2297\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 7s 60ms/step - loss: 1.3936 - acc: 0.2317 - val_loss: 1.3943 - val_acc: 0.2011\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 6s 60ms/step - loss: 1.3921 - acc: 0.2230 - val_loss: 1.3844 - val_acc: 0.2893\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 6s 60ms/step - loss: 1.3918 - acc: 0.2307 - val_loss: 1.3877 - val_acc: 0.2239\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 6s 60ms/step - loss: 1.3893 - acc: 0.2256 - val_loss: 1.3972 - val_acc: 0.1576\n",
      "Epoch 00006: early stopping\n",
      "<tensorflow.python.keras.callbacks.History object at 0x000001291A11F730>\n",
      "Training model completed.\n",
      "Model evaluation will print the following metrics:  ['loss', 'acc']\n",
      "206/206 [==============================] - 1s 7ms/step - loss: 1.3970 - acc: 0.1537\n",
      "[1.3970142602920532, 0.15367111563682556]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "# use Keras to define the structure of the deep neural network   \n",
    "early_stop=EarlyStopping(monitor='acc',mode='max',verbose=1,patience=5,min_delta=0.01)\n",
    "print(\"Creating model structure...\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=embedding_dim))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# fix the weights for the first layer to those provided by the embedding matrix\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "print(\"Creating model structure completed.\")\n",
    "\n",
    "opt = optimizers.RMSprop(lr=0.001)\n",
    "\n",
    "print(\"Training model...\")\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train_seq, y_train_data, epochs=10,\n",
    "                    batch_size=100, validation_split=0.3,\n",
    "                    callbacks=[early_stop],class_weight=weights_dict)\n",
    "print(history)\n",
    "print(\"Training model completed.\")\n",
    "print('Model evaluation will print the following metrics: ', model.metrics_names)\n",
    "evaluation_metrics = model.evaluate(x_test_seq, y_test_dat)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model structure...\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 200)          1600000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200, 85)           97240     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                27200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 1,724,644\n",
      "Trainable params: 124,644\n",
      "Non-trainable params: 1,600,000\n",
      "_________________________________________________________________\n",
      "Creating model structure completed.\n",
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating model structure...\")\n",
    "\n",
    "early_stop=EarlyStopping(monitor='val_acc',mode='max',verbose=1,patience=5,min_delta=0.01)\n",
    "deep_inputs = Input(shape=(embedding_dim,))\n",
    "embedding_layer = Embedding(max_words, embedding_dim, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "LSTM_Layer_1 = LSTM(85,return_sequences=True)(embedding_layer)\n",
    "LSTM_Layer_2 = LSTM(50)(LSTM_Layer_1)\n",
    "dense_layer_1 = Dense(4, activation='softmax')(LSTM_Layer_2)\n",
    "model1 = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "print(\"Creating model structure completed.\")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108/108 [==============================] - 67s 622ms/step - loss: 1.3954 - acc: 0.2444 - val_loss: 1.4113 - val_acc: 0.1216\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 69s 636ms/step - loss: 1.3940 - acc: 0.1839 - val_loss: 1.3868 - val_acc: 0.1664\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 70s 644ms/step - loss: 1.3933 - acc: 0.2265 - val_loss: 1.3889 - val_acc: 0.1209\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 69s 639ms/step - loss: 1.3934 - acc: 0.1845 - val_loss: 1.3953 - val_acc: 0.1242\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 70s 648ms/step - loss: 1.3934 - acc: 0.1594 - val_loss: 1.3875 - val_acc: 0.1283\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 71s 653ms/step - loss: 1.3931 - acc: 0.1817 - val_loss: 1.3862 - val_acc: 0.2115\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 70s 651ms/step - loss: 1.3931 - acc: 0.2065 - val_loss: 1.3913 - val_acc: 0.1680\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 78s 719ms/step - loss: 1.3932 - acc: 0.1765 - val_loss: 1.3965 - val_acc: 0.1361\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 76s 705ms/step - loss: 1.3932 - acc: 0.1573 - val_loss: 1.3872 - val_acc: 0.1797\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 74s 682ms/step - loss: 1.3931 - acc: 0.1884 - val_loss: 1.3902 - val_acc: 0.1504\n",
      "<tensorflow.python.keras.callbacks.History object at 0x0000012921547A00>\n",
      "Training model completed.\n",
      "Model evaluation will print the following metrics:  ['loss', 'acc']\n",
      "206/206 [==============================] - 17s 82ms/step - loss: 1.3892 - acc: 0.1707\n",
      "[1.3892267942428589, 0.17066140472888947]\n",
      "Testing model skill.....\n"
     ]
    }
   ],
   "source": [
    "history2 = model1.fit(x_train_seq, y_train_data, epochs=10,\n",
    "                    batch_size=100, validation_split=0.3,\n",
    "                    callbacks=[early_stop],class_weight=weights_dict)\n",
    "\n",
    "print(history2)\n",
    "print(\"Training model completed.\")\n",
    "print('Model evaluation will print the following metrics: ', model1.metrics_names)\n",
    "evaluation_metrics2 = model1.evaluate(x_test_seq, y_test_dat)\n",
    "print(evaluation_metrics2)\n",
    "\n",
    "print(\"Testing model skill.....\")\n",
    "predictions = model1.predict(x_test_seq).argmax(axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
