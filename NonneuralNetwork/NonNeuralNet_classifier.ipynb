{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "import re\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gc \n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score, recall_score, make_scorer, f1_score, accuracy_score,hamming_loss\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from python_functions import *\n",
    "\n",
    "def load_data(data_filepath):\n",
    "    \n",
    "    \"\"\"\n",
    "        This function takes in the database path and it reads the data\n",
    "        \n",
    "        \n",
    "        return: The messages which are the predators, the response i.e the labels as a dataframe \n",
    "            and an array of all the labels\n",
    "    \"\"\"\n",
    "    df=pd.read_csv(data_filepath, sep=\",\", header=0)\n",
    "    new_df=df[df['sent_length']>20].dropna(subset=['clean_headline']).reset_index(drop=True)\n",
    "    \n",
    "    return new_df \n",
    "\n",
    "def preprocess(data_set,STOPWORDS):\n",
    "    \n",
    "    \"\"\"\n",
    "        This function takes in text and stopwords, it uses the function WORDCOUNT to count the \n",
    "        occurence of each word and add word with only one occurence to the stopwords, then uses function \n",
    "        CLEAN_DATA to remove the combined stopwords and also preprocesed the data\n",
    "        \n",
    "        return: Text devoid of noise.\n",
    "    \"\"\"\n",
    "    # Count of each tokens in the dataset\n",
    "    start = time.time()\n",
    "    print(\"getting less frequent words in dataset ......\")\n",
    "    wordcount=word_count(data_set)\n",
    "    new_stopword=wordcount[wordcount['frequency']==0]['Unigram'].values.tolist()\n",
    "    print('collection of words completed.: {} mins'.format(round((time.time()-start)/60 , 2)))\n",
    "    ## Adding our own stopwords\n",
    "    STOPWORDS.extend(new_stopword)\n",
    "\n",
    "    ## De-noising the dataset and normalisation\n",
    "    print(\"starting data preprocessing ......\")\n",
    "    clean_data=clean_text_process(data_set,stopwords=STOPWORDS)\n",
    "    print('data preprocessing completed.: {} mins'.format(round((time.time()-start)/60 , 2)))\n",
    "\n",
    "    return clean_data\n",
    "\n",
    " \n",
    "STOPWORDS = stopwords.words('english')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=load_data('model_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting less frequent words in dataset ......\n",
      "collection of words completed.: 0.03 mins\n",
      "starting data preprocessing ......\n",
      "data preprocessing completed.: 0.04 mins\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "df['new_cln_data']=preprocess(df['clean_headline'],STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_headline</th>\n",
       "      <th>category</th>\n",
       "      <th>sent_length</th>\n",
       "      <th>new_cln_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>threat right wing supreme court anal zing trum...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>68</td>\n",
       "      <td>threat right wing supreme court anal zing trum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hillary clinton really wants think tough wall ...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>52</td>\n",
       "      <td>hillary clinton really wants think tough wall ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>photo barack obama irish immigrant rd great gr...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>84</td>\n",
       "      <td>photo barack obama irish immigrant rd great gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rethinking battlefield</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>22</td>\n",
       "      <td>rethinking battlefield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scalia utter moral failure exposed</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>34</td>\n",
       "      <td>scalia utter moral failure exposed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21967</th>\n",
       "      <td>spending lot time digital devices</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>33</td>\n",
       "      <td>spending lot time digital devices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21968</th>\n",
       "      <td>zone use breath posture passion get flow state</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>46</td>\n",
       "      <td>zone use breath posture passion get flow state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21969</th>\n",
       "      <td>road term jerry brown ash rolls arnold schwarz...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>53</td>\n",
       "      <td>road term jerry brown ash rolls arnold schwarz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21970</th>\n",
       "      <td>shallow salesmanship carly fi</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>29</td>\n",
       "      <td>shallow salesmanship carly fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21971</th>\n",
       "      <td>gal ga dot surprises college student first won...</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>67</td>\n",
       "      <td>gal ga dot surprises college student first won...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21972 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_headline  category  \\\n",
       "0      threat right wing supreme court anal zing trum...  POLITICS   \n",
       "1      hillary clinton really wants think tough wall ...  POLITICS   \n",
       "2      photo barack obama irish immigrant rd great gr...  POLITICS   \n",
       "3                                 rethinking battlefield    TRAVEL   \n",
       "4                     scalia utter moral failure exposed  WELLNESS   \n",
       "...                                                  ...       ...   \n",
       "21967                  spending lot time digital devices  WELLNESS   \n",
       "21968     zone use breath posture passion get flow state  POLITICS   \n",
       "21969  road term jerry brown ash rolls arnold schwarz...  POLITICS   \n",
       "21970                      shallow salesmanship carly fi  POLITICS   \n",
       "21971  gal ga dot surprises college student first won...  WELLNESS   \n",
       "\n",
       "       sent_length                                       new_cln_data  \n",
       "0               68  threat right wing supreme court anal zing trum...  \n",
       "1               52  hillary clinton really wants think tough wall ...  \n",
       "2               84  photo barack obama irish immigrant rd great gr...  \n",
       "3               22                             rethinking battlefield  \n",
       "4               34                 scalia utter moral failure exposed  \n",
       "...            ...                                                ...  \n",
       "21967           33                  spending lot time digital devices  \n",
       "21968           46     zone use breath posture passion get flow state  \n",
       "21969           53  road term jerry brown ash rolls arnold schwarz...  \n",
       "21970           29                      shallow salesmanship carly fi  \n",
       "21971           67  gal ga dot surprises college student first won...  \n",
       "\n",
       "[21972 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POLITICS': 0, 'TRAVEL': 1, 'WELLNESS': 2, 'ENTERTAINMENT': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Changing label to categorical values\n",
    "news_labels=df['category'].unique()\n",
    "news_labels_dict={}\n",
    "for index in range(len(news_labels)):\n",
    "    news_labels_dict[news_labels[index]]=index\n",
    "news_labels_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    2\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df['category'].apply(lambda x: news_labels_dict[x])\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting to training and test split.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['new_cln_data'],labels, \n",
    "                                                               test_size=0.25, \n",
    "                                                               random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akino\\anaconda3\\envs\\DeepLearning_tf\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1 2 3], y=15347    1\n",
      "311      0\n",
      "14465    1\n",
      "684      2\n",
      "12460    3\n",
      "        ..\n",
      "6400     0\n",
      "15288    0\n",
      "11513    3\n",
      "1688     0\n",
      "5994     0\n",
      "Name: category, Length: 16479, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.587360992301112,\n",
       " 1: 1.9188402421984163,\n",
       " 2: 1.0779042386185242,\n",
       " 3: 1.1784181922196797}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculation of the class weight\n",
    "weights= compute_class_weight(\n",
    "           'balanced',\n",
    "            np.unique(y_train), \n",
    "            y_train)\n",
    "\n",
    "weights_dict = dict(zip( np.unique(y_train),weights))\n",
    "weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of model with different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(sublinear_tf=True,norm='l2',\n",
    "                        ngram_range=(1, 2),)),\n",
    "                              \n",
    "                ('mnb', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    # specify parameters for grid search\n",
    "parameters = {\n",
    "    'tfidf__min_df': np.array([10, 20,30,40,50]), \n",
    "    'tfidf__sublinear_tf': np.array([True,False]),\n",
    "    'mnb__alpha': np.linspace(0.1, 1.5, 10),\n",
    "               \n",
    "        \n",
    "    }\n",
    "\n",
    "    # create grid search object\n",
    "model =  GridSearchCV(pipeline, param_grid=parameters, scoring='accuracy')\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tCLASSIFICATIION METRICS\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     POLITICS       0.43      0.99      0.60      2371\n",
      "       TRAVEL       0.00      0.00      0.00       681\n",
      "     WELLNESS       0.23      0.01      0.01      1289\n",
      "ENTERTAINMENT       0.19      0.00      0.01      1152\n",
      "\n",
      "     accuracy                           0.43      5493\n",
      "    macro avg       0.21      0.25      0.16      5493\n",
      " weighted avg       0.28      0.43      0.26      5493\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akino\\anaconda3\\envs\\DeepLearning_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(y_test, y_pred, \n",
    "                                    target_names= df['category'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(sublinear_tf=True,norm='l2',\n",
    "                        ngram_range=(1, 2),)),\n",
    "                              \n",
    "                ('rfc', RandomForestClassifier())\n",
    "    ])\n",
    "    # specify parameters for grid search\n",
    "parameters2 = {\n",
    "    'tfidf__min_df': np.array([10, 20,30,40,50]), \n",
    "    'tfidf__sublinear_tf': np.array([True,False]),\n",
    "    'rfc__n_estimators': np.arange(100, 201, 10),\n",
    "    'rfc__class_weight': (None,weights_dict)\n",
    "               \n",
    "        \n",
    "    }\n",
    "\n",
    "    # create grid search object\n",
    "model2 =  GridSearchCV(pipeline2, param_grid=parameters2, scoring='accuracy')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_train, y_train)\n",
    "y_pred2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(y_test, y_pred2, \n",
    "                                    target_names= df['category'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline3 = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(sublinear_tf=True,norm='l2',\n",
    "                        ngram_range=(1, 2),)),\n",
    "                              \n",
    "                ('Lsvc', LinearSVC())\n",
    "    ])\n",
    "    # specify parameters for grid search\n",
    "parameters3 = {\n",
    "    'tfidf__min_df': np.array([10, 20,30,40,50]), \n",
    "    'tfidf__sublinear_tf': np.array([True,False]),\n",
    "    'Lsvc__C': np.linspace(0.1, 1, 6),\n",
    "    'Lsvc__class_weight': (None,weights_dict),\n",
    "    'Lsvc__loss':np.array(['hinge', 'squared_hinge'])\n",
    "               \n",
    "        \n",
    "    }\n",
    "\n",
    "    # create grid search object\n",
    "model3 =  GridSearchCV(pipeline3, param_grid=parameters3, scoring='accuracy')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(X_train, y_train)\n",
    "y_pred3 = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
    "print(metrics.classification_report(y_test, y_pred3, \n",
    "                                    target_names= df['category'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
